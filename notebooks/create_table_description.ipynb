{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.auth\n",
    "import os\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_connection_v1 as bq_connection\n",
    "from abc import ABC\n",
    "from datetime import datetime\n",
    "import google.auth\n",
    "import pandas as pd\n",
    "from google.cloud.exceptions import NotFound\n",
    "from dotenv import load_dotenv\n",
    "from pandas_gbq import to_gbq\n",
    "from abc import ABC\n",
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "from vertexai.language_models import CodeGenerationModel\n",
    "from vertexai.language_models import CodeChatModel\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "from vertexai.generative_models import HarmCategory,HarmBlockThreshold\n",
    "from vertexai.generative_models import GenerationConfig\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "import time\n",
    "load_dotenv()\n",
    "import yaml\n",
    "\n",
    "\n",
    "# project_id = os.getenv(\"PROJECT_ID\")\n",
    "# dataset = os.getenv(\"DATABASE_ID\")\n",
    "\n",
    "with open('../sql_config.yml' , 'r') as f:\n",
    "    sql_config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "project_id = sql_config['bigquery']['project_id']\n",
    "dataset = sql_config['bigquery']['dataset_id']\n",
    "location = sql_config['bigquery']['region']\n",
    "\n",
    "vertexai.init(project=sql_config['bigquery']['project_id'], location=sql_config['bigquery']['region'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def return_table_schema_sql(project_id, dataset, table_names=None):\n",
    "    \"\"\"\n",
    "    Returns the SQL query to get table schema info, optionally filtering by specific tables.\n",
    "    \"\"\"\n",
    "    user_dataset = f\"{project_id}.{dataset}\"\n",
    "\n",
    "    table_filter_clause = \"\"\n",
    "    if table_names:\n",
    "        # Extract individual table names from the input string\n",
    "        #table_names = [name.strip() for name in table_names[1:-1].split(\",\")]  # Handle the string as a list\n",
    "        formatted_table_names = [f\"'{name}'\" for name in table_names]\n",
    "        table_filter_clause = f\"\"\"AND TABLE_NAME IN ({', '.join(formatted_table_names)})\"\"\"\n",
    "\n",
    "\n",
    "    table_schema_sql = f\"\"\"\n",
    "    (SELECT\n",
    "        TABLE_CATALOG as project_id, TABLE_SCHEMA as table_schema , TABLE_NAME as table_name,  OPTION_VALUE as table_description,\n",
    "        (SELECT STRING_AGG(column_name, ', ') from `{user_dataset}.INFORMATION_SCHEMA.COLUMNS` where TABLE_NAME= t.TABLE_NAME and TABLE_SCHEMA=t.TABLE_SCHEMA) as table_columns\n",
    "    FROM\n",
    "        `{user_dataset}.INFORMATION_SCHEMA.TABLE_OPTIONS` as t\n",
    "    WHERE\n",
    "        OPTION_NAME = \"description\"\n",
    "        {table_filter_clause}\n",
    "    ORDER BY\n",
    "        project_id, table_schema, table_name)\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    (SELECT\n",
    "        TABLE_CATALOG as project_id, TABLE_SCHEMA as table_schema , TABLE_NAME as table_name,  \"NA\" as table_description,\n",
    "        (SELECT STRING_AGG(column_name, ', ') from `{user_dataset}.INFORMATION_SCHEMA.COLUMNS` where TABLE_NAME= t.TABLE_NAME and TABLE_SCHEMA=t.TABLE_SCHEMA) as table_columns\n",
    "    FROM\n",
    "        `{user_dataset}.INFORMATION_SCHEMA.TABLES` as t \n",
    "    WHERE \n",
    "        NOT EXISTS (SELECT 1   FROM\n",
    "        `{user_dataset}.INFORMATION_SCHEMA.TABLE_OPTIONS`  \n",
    "    WHERE\n",
    "        OPTION_NAME = \"description\" AND  TABLE_NAME= t.TABLE_NAME and TABLE_SCHEMA=t.TABLE_SCHEMA)\n",
    "        {table_filter_clause}\n",
    "    ORDER BY\n",
    "        project_id, table_schema, table_name)\n",
    "    \"\"\"\n",
    "    return table_schema_sql\n",
    "\n",
    "def return_column_schema_sql(project_id, dataset, table_names=None):\n",
    "    \"\"\"\n",
    "    Returns the SQL query to get column schema info, optionally filtering by specific tables.\n",
    "    \"\"\"\n",
    "    user_dataset = f\"{project_id}.{dataset}\"\n",
    "    \n",
    "    table_filter_clause = \"\"\n",
    "    if table_names:\n",
    "        # table_names = [name.strip() for name in table_names[1:-1].split(\",\")]  # Handle the string as a list\n",
    "        formatted_table_names = [f\"'{name}'\" for name in table_names]\n",
    "        table_filter_clause = f\"\"\"AND C.TABLE_NAME IN ({', '.join(formatted_table_names)})\"\"\"\n",
    "\n",
    "    column_schema_sql = f\"\"\"\n",
    "    SELECT\n",
    "        C.TABLE_CATALOG as project_id, C.TABLE_SCHEMA as table_schema, C.TABLE_NAME as table_name, C.COLUMN_NAME as column_name,\n",
    "        C.DATA_TYPE as data_type, C.DESCRIPTION as column_description, CASE WHEN T.CONSTRAINT_TYPE=\"PRIMARY KEY\" THEN \"This Column is a Primary Key for this table\" WHEN \n",
    "        T.CONSTRAINT_TYPE = \"FOREIGN_KEY\" THEN \"This column is Foreign Key\" ELSE NULL END as column_constraints\n",
    "    FROM\n",
    "        `{user_dataset}.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS` C \n",
    "    LEFT JOIN \n",
    "        `{user_dataset}.INFORMATION_SCHEMA.TABLE_CONSTRAINTS` T \n",
    "        ON C.TABLE_CATALOG = T.TABLE_CATALOG AND\n",
    "           C.TABLE_SCHEMA = T.TABLE_SCHEMA AND \n",
    "           C.TABLE_NAME = T.TABLE_NAME AND  \n",
    "           T.ENFORCED ='YES'\n",
    "    LEFT JOIN \n",
    "        `{user_dataset}.INFORMATION_SCHEMA.KEY_COLUMN_USAGE` K\n",
    "        ON K.CONSTRAINT_NAME=T.CONSTRAINT_NAME AND C.COLUMN_NAME = K.COLUMN_NAME \n",
    "    WHERE\n",
    "        1=1\n",
    "        {table_filter_clause} \n",
    "    ORDER BY\n",
    "        project_id, table_schema, table_name, column_name;\n",
    "\"\"\"\n",
    "\n",
    "    return column_schema_sql\n",
    "\n",
    "\n",
    "#Create Descrition Agent\n",
    "\"\"\"\n",
    "Provides the base class for all Agents \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Agent(ABC):\n",
    "    \"\"\"\n",
    "    The core class for all Agents\n",
    "    \"\"\"\n",
    "\n",
    "    agentType: str = \"Agent\"\n",
    "\n",
    "    def __init__(self,\n",
    "                model_id:str):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            PROJECT_ID (str | None): GCP Project Id.\n",
    "            dataset_name (str): \n",
    "            TODO\n",
    "        \"\"\"\n",
    "\n",
    "        self.model_id = model_id \n",
    "\n",
    "        if model_id == 'code-bison-32k':\n",
    "            self.model = CodeGenerationModel.from_pretrained('code-bison-32k')\n",
    "        elif model_id == 'text-bison-32k':\n",
    "            self.model = TextGenerationModel.from_pretrained('text-bison-32k')\n",
    "        elif model_id == 'codechat-bison-32k':\n",
    "            self.model = CodeChatModel.from_pretrained(\"codechat-bison-32k\")\n",
    "        elif model_id == 'gemini-1.0-pro' or model_id == 'gemini-1.5-flash-001':\n",
    "            self.model = GenerativeModel(\"gemini-1.5-flash-001\")\n",
    "        else:\n",
    "            raise ValueError(\"Please specify a compatible model.\")\n",
    "\n",
    "    def generate_llm_response(self,prompt):\n",
    "        context_query = self.model.generate_content(prompt,stream=False)\n",
    "        return str(context_query.candidates[0].text).replace(\"```sql\", \"\").replace(\"```\", \"\")\n",
    "\n",
    "class EmbedderAgent(Agent, ABC): \n",
    "    \"\"\" \n",
    "    This Agent generates embeddings \n",
    "    \"\"\" \n",
    "\n",
    "    agentType: str = \"EmbedderAgent\"\n",
    "\n",
    "    def __init__(self, mode, embeddings_model='textembedding-gecko@002'): \n",
    "        if mode == 'vertex': \n",
    "            self.mode = mode \n",
    "            self.model = TextEmbeddingModel.from_pretrained(embeddings_model)\n",
    "\n",
    "        else: raise ValueError('EmbedderAgent mode must be vertex')\n",
    "\n",
    "\n",
    "\n",
    "    def create(self, question): \n",
    "        \"\"\"Text embedding with a Large Language Model.\"\"\"\n",
    "\n",
    "        if self.mode == 'vertex': \n",
    "            if isinstance(question, str): \n",
    "                embeddings = self.model.get_embeddings([question])\n",
    "                for embedding in embeddings:\n",
    "                    vector = embedding.values\n",
    "                return vector\n",
    "            \n",
    "            elif isinstance(question, list):  \n",
    "                vector = list() \n",
    "                for q in question: \n",
    "                    embeddings = self.model.get_embeddings([q])\n",
    "\n",
    "                    for embedding in embeddings:\n",
    "                        vector.append(embedding.values) \n",
    "                return vector\n",
    "            \n",
    "            else: raise ValueError('Input must be either str or list')\n",
    "\n",
    "\n",
    "class DescriptionAgent(Agent, ABC): \n",
    "    \"\"\" \n",
    "    Generates table and column descriptions. \n",
    "    \"\"\" \n",
    "\n",
    "    agentType: str = \"DescriptionAgent\"\n",
    "\n",
    "    def generate_llm_response(self,prompt):\n",
    "        context_query = self.model.generate_content(prompt,stream=False)\n",
    "        return str(context_query.candidates[0].text).replace(\"```sql\", \"\").replace(\"```\", \"\")\n",
    "\n",
    "\n",
    "    def generate_missing_descriptions(self,source,table_desc_df, column_name_df):\n",
    "        llm_generated=0\n",
    "        for index, row in table_desc_df.iterrows():\n",
    "            print(f\"Table Description for {row['table_name']}\")\n",
    "\n",
    "            if row['table_description'] is None or row['table_description']=='NA':\n",
    "                q=f\"table_name == '{row['table_name']}' and table_schema == '{row['table_schema']}'\"\n",
    "                if source=='bigquery':\n",
    "                    context_prompt = f\"\"\"\n",
    "                        Generate table description short and crisp for the table {row['project_id']}.{row['table_schema']}.{row['table_name']}\n",
    "                        Remember that these desciprtion should help LLMs to help build better SQL for any quries related to this table.\n",
    "                        Parameters:\n",
    "                        - column metadata: {column_name_df.query(q).to_markdown(index = False)}\n",
    "                        - table metadata: {table_desc_df.query(q).to_markdown(index = False)}\n",
    "                        \n",
    "                        DO NOT generate description more than two lines\n",
    "                    \"\"\"\n",
    "\n",
    "\n",
    "                table_desc_df.at[index,'table_description']=self.generate_llm_response(context_prompt)\n",
    "                # print(row['table_description'])\n",
    "                llm_generated=llm_generated+1\n",
    "                time.sleep(13)\n",
    "        print(\"\\nLLM generated \"+ str(llm_generated) + \" Table Descriptions\")\n",
    "        llm_generated = 0\n",
    "\n",
    "        \n",
    "        for index, row in column_name_df.iterrows():\n",
    "            # print(row['column_description'])\n",
    "            print(f\"column_description for {row['column_name']} at Table {row['table_name']}\")\n",
    "            if row['column_description'] is None or row['column_description']=='':\n",
    "                q=f\"table_name == '{row['table_name']}' and table_schema == '{row['table_schema']}'\"\n",
    "                if source=='bigquery':\n",
    "                    context_prompt = f\"\"\"\n",
    "                    Generate short and crisp description for the column {row['project_id']}.{row['table_schema']}.{row['table_name']}.{row['column_name']}\n",
    "\n",
    "                    Remember that this description should help LLMs to help generate better SQL for any queries related to these columns.\n",
    "\n",
    "                    Consider the below information to generate a good comment\n",
    "\n",
    "                    Name of the column : {row['column_name']}\n",
    "                    Data type of the column is : {row['data_type']}\n",
    "                    Details of the table of this column are below:\n",
    "                    {table_desc_df.query(q).to_markdown(index=False)}\n",
    "                    Column Contrainst of this column are : {row['column_constraints']}\n",
    "\n",
    "                    DO NOT generate description more than two lines\n",
    "                \"\"\"\n",
    "                \n",
    "\n",
    "                column_name_df.at[index,'column_description']=self.generate_llm_response(prompt=context_prompt)\n",
    "                # print(row['column_description'])\n",
    "                llm_generated=llm_generated+1\n",
    "                time.sleep(13)\n",
    "        print(\"\\nLLM generated \"+ str(llm_generated) + \" Column Descriptions\")\n",
    "        \n",
    "        return table_desc_df,column_name_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_embedding_chunked(textinput, batch_size): \n",
    "    embedder = EmbedderAgent('vertex')\n",
    "\n",
    "    for i in range(0, len(textinput), batch_size):\n",
    "        request = [x[\"content\"] for x in textinput[i : i + batch_size]]\n",
    "        response = embedder.create(request) # Vertex Textmodel Embedder \n",
    "\n",
    "        # Store the retrieved vector embeddings for each chunk back.\n",
    "        for x, e in zip(textinput[i : i + batch_size], response):\n",
    "            x[\"embedding\"] = e\n",
    "\n",
    "    # Store the generated embeddings in a pandas dataframe.\n",
    "    out_df = pd.DataFrame(textinput)\n",
    "    return out_df\n",
    "\n",
    "\n",
    "#Function to generate embeddings:\n",
    "def retrieve_embeddings(): \n",
    "    \"\"\" Augment all the DB schema blocks to create document for embedding \"\"\"\n",
    "\n",
    "    #TABLE EMBEDDINGS\n",
    "    table_details_chunked = []\n",
    "\n",
    "    for _, row_aug in table_desc_df.iterrows():\n",
    "        cur_project_name =str(row_aug['project_id'])\n",
    "        cur_table_name = str(row_aug['table_name'])\n",
    "        cur_table_schema = str(row_aug['table_schema'])\n",
    "        curr_col_names = str(row_aug['table_columns'])\n",
    "        curr_tbl_desc = str(row_aug['table_description'])\n",
    "\n",
    "\n",
    "        table_detailed_description=f\"\"\"\n",
    "        Full Table Name : {cur_project_name}.{cur_table_schema}.{cur_table_name} |\n",
    "        Table Columns List: [{curr_col_names}] |\n",
    "        Table Description: {curr_tbl_desc} \"\"\"\n",
    "\n",
    "        r = {\"table_schema\": cur_table_schema,\"table_name\": cur_table_name,\"content\": table_detailed_description}\n",
    "        table_details_chunked.append(r)\n",
    "\n",
    "    table_details_embeddings = get_embedding_chunked(table_details_chunked, 10)\n",
    "\n",
    "\n",
    "    ### COLUMN EMBEDDING ###\n",
    "    \"\"\"\n",
    "    This SQL returns a df containing the cols table_schema, table_name, column_name, data_type, column_description, table_description, primary_key, column_constraints\n",
    "    for the schema specified above, e.g. 'retail'\n",
    "    \"\"\"\n",
    "\n",
    "    column_details_chunked = []\n",
    "\n",
    "    for _, row_aug in column_name_df.iterrows():\n",
    "        cur_project_name =str(row_aug['project_id'])\n",
    "        cur_table_name = str(row_aug['table_name'])\n",
    "        cur_table_owner = str(row_aug['table_schema'])\n",
    "        curr_col_name = str(row_aug['table_schema'])+'.'+str(row_aug['table_name'])+'.'+str(row_aug['column_name'])\n",
    "        curr_col_datatype = str(row_aug['data_type'])\n",
    "        curr_col_description = str(row_aug['column_description'])\n",
    "        curr_col_constraints = str(row_aug['column_constraints'])\n",
    "        curr_column_name = str(row_aug['column_name'])\n",
    "\n",
    "\n",
    "        column_detailed_description=f\"\"\"\n",
    "        Column Name: {curr_col_name}|\n",
    "        Full Table Name : {cur_project_name}.{cur_table_schema}.{cur_table_name} |\n",
    "        Data type: {curr_col_datatype}|\n",
    "        Column description: {curr_col_description}|\n",
    "        Column Constraints: {curr_col_constraints} \"\"\"\n",
    "\n",
    "        r = {\"table_schema\": cur_table_owner,\"table_name\": cur_table_name,\"column_name\":curr_column_name, \"content\": column_detailed_description}\n",
    "        column_details_chunked.append(r)\n",
    "\n",
    "    column_details_embeddings = get_embedding_chunked(column_details_chunked, 10)\n",
    "\n",
    "\n",
    "    return table_details_embeddings, column_details_embeddings\n",
    "\n",
    "\n",
    "async def store_schema_embeddings(table_details_embeddings, \n",
    "                            tablecolumn_details_embeddings, \n",
    "                            project_id,\n",
    "                            schema):\n",
    "    \"\"\" \n",
    "    Store the vectorised table and column details in the DB table.\n",
    "    This code may run for a few minutes.  \n",
    "    \"\"\"\n",
    "         \n",
    "    client=bigquery.Client(project=project_id)\n",
    "\n",
    "    #Store table embeddings\n",
    "    client.query_and_wait(f'''CREATE TABLE IF NOT EXISTS `{project_id}.{schema}.table_details_embeddings` (\n",
    "        source_type string NOT NULL, table_schema string NOT NULL, table_name string NOT NULL, content string, embedding ARRAY<FLOAT64>)''')\n",
    "    #job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
    "    table_details_embeddings['source_type']='BigQuery'\n",
    "    for _, row in table_details_embeddings.iterrows():\n",
    "        client.query_and_wait(f'''DELETE FROM `{project_id}.{schema}.table_details_embeddings`\n",
    "                WHERE table_schema= '{row[\"table_schema\"]}' and table_name= '{row[\"table_name\"]}' '''\n",
    "                    )\n",
    "    client.load_table_from_dataframe(table_details_embeddings,f'{project_id}.{schema}.table_details_embeddings')\n",
    "\n",
    "\n",
    "    #Store column embeddings\n",
    "    client.query_and_wait(f'''CREATE TABLE IF NOT EXISTS `{project_id}.{schema}.tablecolumn_details_embeddings` (\n",
    "        source_type string NOT NULL, table_schema string NOT NULL, table_name string NOT NULL, column_name string NOT NULL,\n",
    "        content string, embedding ARRAY<FLOAT64>)''')\n",
    "    #job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
    "    tablecolumn_details_embeddings['source_type']='BigQuery'\n",
    "    for _, row in tablecolumn_details_embeddings.iterrows():\n",
    "        client.query_and_wait(f'''DELETE FROM `{project_id}.{schema}.tablecolumn_details_embeddings`\n",
    "                WHERE table_schema= '{row[\"table_schema\"]}' and table_name= '{row[\"table_name\"]}' and column_name= '{row[\"column_name\"]}' '''\n",
    "                    )\n",
    "    client.load_table_from_dataframe(tablecolumn_details_embeddings,f'{project_id}.{schema}.tablecolumn_details_embeddings')\n",
    "\n",
    "    return \"Embeddings are stored successfully\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###Get the Tables and Columns in Database\n",
    "client = bigquery.Client(project=project_id)\n",
    "table_schema_sql = return_table_schema_sql(project_id, dataset)\n",
    "table_desc_df = client.query_and_wait(table_schema_sql).to_dataframe()\n",
    "column_schema_sql = return_column_schema_sql(project_id, dataset)\n",
    "column_desc_df = client.query_and_wait(column_schema_sql).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Description for customer_counterfactual_recommendations\n",
      "Table Description for customer_shap_data\n",
      "Table Description for global_shap_summary\n",
      "Table Description for customer_data\n",
      "\n",
      "LLM generated 0 Table Descriptions\n",
      "column_description for changes at Table customer_counterfactual_recommendations\n",
      "column_description for customerid at Table customer_counterfactual_recommendations\n",
      "column_description for activesubs at Table customer_data\n",
      "column_description for adjustmentstocreditrating at Table customer_data\n",
      "column_description for agehh1 at Table customer_data\n",
      "column_description for avg_call_duration at Table customer_data\n"
     ]
    },
    {
     "ename": "PermissionDenied",
     "evalue": "403 Vertex AI API has not been used in project mlchatagent-429005 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/aiplatform.googleapis.com/overview?project=mlchatagent-429005 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry. [links {\n  description: \"Google developers console API activation\"\n  url: \"https://console.developers.google.com/apis/api/aiplatform.googleapis.com/overview?project=mlchatagent-429005\"\n}\n, reason: \"SERVICE_DISABLED\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"aiplatform.googleapis.com\"\n}\nmetadata {\n  key: \"consumer\"\n  value: \"projects/mlchatagent-429005\"\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32md:\\Work\\Github\\mlchatagent\\venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32md:\\Work\\Github\\mlchatagent\\venv\\Lib\\site-packages\\grpc\\_channel.py:1181\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1175\u001b[0m (\n\u001b[0;32m   1176\u001b[0m     state,\n\u001b[0;32m   1177\u001b[0m     call,\n\u001b[0;32m   1178\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[0;32m   1179\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[0;32m   1180\u001b[0m )\n\u001b[1;32m-> 1181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Work\\Github\\mlchatagent\\venv\\Lib\\site-packages\\grpc\\_channel.py:1006\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[1;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1006\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[1;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.PERMISSION_DENIED\n\tdetails = \"Vertex AI API has not been used in project mlchatagent-429005 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/aiplatform.googleapis.com/overview?project=mlchatagent-429005 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.250.138.95:443 {grpc_message:\"Vertex AI API has not been used in project mlchatagent-429005 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/aiplatform.googleapis.com/overview?project=mlchatagent-429005 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry.\", grpc_status:7, created_time:\"2024-07-10T06:18:15.0066179+00:00\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mPermissionDenied\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m descriptor \u001b[38;5;241m=\u001b[39m DescriptionAgent(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemini-1.5-flash-001\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#GENERATE MISSING DESCRIPTIONS\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m table_desc_df,column_name_df\u001b[38;5;241m=\u001b[39m \u001b[43mdescriptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_missing_descriptions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbigquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtable_desc_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcolumn_desc_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 225\u001b[0m, in \u001b[0;36mDescriptionAgent.generate_missing_descriptions\u001b[1;34m(self, source, table_desc_df, column_name_df)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbigquery\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    208\u001b[0m     context_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124m    Generate short and crisp description for the column \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproject_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable_schema\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumn_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124m    DO NOT generate description more than two lines\u001b[39m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m--> 225\u001b[0m column_name_df\u001b[38;5;241m.\u001b[39mat[index,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumn_description\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_llm_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# print(row['column_description'])\u001b[39;00m\n\u001b[0;32m    227\u001b[0m llm_generated\u001b[38;5;241m=\u001b[39mllm_generated\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[2], line 171\u001b[0m, in \u001b[0;36mDescriptionAgent.generate_llm_response\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_llm_response\u001b[39m(\u001b[38;5;28mself\u001b[39m,prompt):\n\u001b[1;32m--> 171\u001b[0m     context_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(context_query\u001b[38;5;241m.\u001b[39mcandidates[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```sql\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Work\\Github\\mlchatagent\\venv\\Lib\\site-packages\\vertexai\\generative_models\\_generative_models.py:407\u001b[0m, in \u001b[0;36m_GenerativeModel.generate_content\u001b[1;34m(self, contents, generation_config, safety_settings, tools, tool_config, stream)\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_content_streaming(\n\u001b[0;32m    400\u001b[0m         contents\u001b[38;5;241m=\u001b[39mcontents,\n\u001b[0;32m    401\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    404\u001b[0m         tool_config\u001b[38;5;241m=\u001b[39mtool_config,\n\u001b[0;32m    405\u001b[0m     )\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafety_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtool_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Work\\Github\\mlchatagent\\venv\\Lib\\site-packages\\vertexai\\generative_models\\_generative_models.py:496\u001b[0m, in \u001b[0;36m_GenerativeModel._generate_content\u001b[1;34m(self, contents, generation_config, safety_settings, tools, tool_config)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generates content.\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;124;03m    A single GenerationResponse object\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    489\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[0;32m    490\u001b[0m     contents\u001b[38;5;241m=\u001b[39mcontents,\n\u001b[0;32m    491\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m     tool_config\u001b[38;5;241m=\u001b[39mtool_config,\n\u001b[0;32m    495\u001b[0m )\n\u001b[1;32m--> 496\u001b[0m gapic_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prediction_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_response(gapic_response)\n",
      "File \u001b[1;32md:\\Work\\Github\\mlchatagent\\venv\\Lib\\site-packages\\google\\cloud\\aiplatform_v1beta1\\services\\prediction_service\\client.py:2102\u001b[0m, in \u001b[0;36mPredictionServiceClient.generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m   2099\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m   2101\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m-> 2102\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2107\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2109\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32md:\\Work\\Github\\mlchatagent\\venv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Work\\Github\\mlchatagent\\venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mPermissionDenied\u001b[0m: 403 Vertex AI API has not been used in project mlchatagent-429005 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/aiplatform.googleapis.com/overview?project=mlchatagent-429005 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry. [links {\n  description: \"Google developers console API activation\"\n  url: \"https://console.developers.google.com/apis/api/aiplatform.googleapis.com/overview?project=mlchatagent-429005\"\n}\n, reason: \"SERVICE_DISABLED\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"aiplatform.googleapis.com\"\n}\nmetadata {\n  key: \"consumer\"\n  value: \"projects/mlchatagent-429005\"\n}\n]"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(table_desc_df.head())\n",
    "descriptor = DescriptionAgent('gemini-1.5-flash-001')\n",
    "#GENERATE MISSING DESCRIPTIONS\n",
    "table_desc_df,column_name_df= descriptor.generate_missing_descriptions('bigquery',table_desc_df,column_desc_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table and Column embeddings are saved to vector store\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##Get EMbeddings added to table adn columns description\n",
    "table_schema_embeddings, col_schema_embeddings = retrieve_embeddings()\n",
    "#store the embeddings back to the vector db.\n",
    "await(store_schema_embeddings(table_details_embeddings=table_schema_embeddings, \n",
    "                                tablecolumn_details_embeddings=col_schema_embeddings, \n",
    "                                project_id=project_id,\n",
    "                                schema='telecom_churn'                               \n",
    "                                ))\n",
    "print(\"Table and Column embeddings are saved to vector store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Known Good SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = EmbedderAgent('vertex')\n",
    "async def setup_kgq_table( project_id,\n",
    "                            schema):\n",
    "    \"\"\" \n",
    "    This function sets up or refreshes the Vector Store for Known Good Queries (KGQ)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create BQ Client\n",
    "    client=bigquery.Client(project=project_id)\n",
    "\n",
    "    # Delete an old table\n",
    "    client.query_and_wait(f'''DROP TABLE IF EXISTS `{project_id}.{schema}.example_prompt_sql_embeddings`''')\n",
    "    # Create a new emptry table\n",
    "    client.query_and_wait(f'''CREATE TABLE IF NOT EXISTS `{project_id}.{schema}.example_prompt_sql_embeddings` (\n",
    "                            table_schema string NOT NULL, example_user_question string NOT NULL, example_generated_sql string NOT NULL,\n",
    "                            embedding ARRAY<FLOAT64>)''')\n",
    "        \n",
    "\n",
    "async def store_kgq_embeddings(df_kgq, \n",
    "                            project_id,\n",
    "                            schema\n",
    "                            ):\n",
    "    \"\"\" \n",
    "    Create and save the Known Good Query Embeddings to Vector Store  \n",
    "    \"\"\"\n",
    "\n",
    "    client=bigquery.Client(project=project_id)\n",
    "    \n",
    "    example_sql_details_chunked = []\n",
    "\n",
    "    for _, row_aug in df_kgq.iterrows():\n",
    "\n",
    "        example_user_question = str(row_aug['prompt'])\n",
    "        example_generated_sql = str(row_aug['sql'])\n",
    "        example_database_name = str(row_aug['database_name'])\n",
    "        emb =  embedder.create(example_user_question)\n",
    "        \n",
    "\n",
    "        r = {\"example_database_name\":example_database_name,\"example_user_question\": example_user_question,\"example_generated_sql\": example_generated_sql,\"embedding\": emb}\n",
    "        example_sql_details_chunked.append(r)\n",
    "\n",
    "    example_prompt_sql_embeddings = pd.DataFrame(example_sql_details_chunked)\n",
    "\n",
    "    for _, row in example_prompt_sql_embeddings.iterrows():\n",
    "            print(f\"Example SQL Embeddings for {row['example_user_question']}\")\n",
    "            print(f\"Example SQL Embeddings for {row['example_database_name']}\")\n",
    "            client.query_and_wait(f'''DELETE FROM `{project_id}.{schema}.example_prompt_sql_embeddings`\n",
    "                        WHERE table_schema= '{row[\"example_database_name\"]}' and example_user_question= '{row[\"example_user_question\"]}' '''\n",
    "                            )\n",
    "                # embedding=np.array(row[\"embedding\"])\n",
    "            cleaned_sql = row[\"example_generated_sql\"].replace(\"\\n\", \" \")\n",
    "            client.query_and_wait(f'''INSERT INTO `{project_id}.{schema}.example_prompt_sql_embeddings` \n",
    "                VALUES (\"{row[\"example_database_name\"]}\",\"{row[\"example_user_question\"]}\" , \n",
    "                \"{cleaned_sql}\",{row[\"embedding\"]} )''')\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Setup the KGSQL Table\n",
    "await(setup_kgq_table( project_id,'telecom_churn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example SQL Embeddings for What is the average monthly charges for customers who have churned?\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for Tell me about customerID 3114822?\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for Who is customer 3114822?\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for Tell me SHAP contribution of customerID 3114822?\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for Get the data of customerID 3114822?\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for Explain to me the contribution towards churn prediction of customerID 3114822?\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for Explain to me the main reasons for churn of customerID 3114822?\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for Tell me some action recommendations for customerID 3334558?\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for What are some actions that can be performed to reduce churn probability for customerID 3334558?\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for Give a summary of shap contribution of age and revenue customers in service city hou\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for What is the average monthly revenue and churn rate for each service city?\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for Which customers have the highest SHAP contributions for revenue and what are their respective ages and service cities?\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for What is the total number of inbound calls for customers who have been in service for more than 12 months and have a high credit rating\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for What are the top 5 cities with the highest average call duration, and what is the corresponding average call duration?\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for How many customers have at least one child in their household and have opted out of mailings?\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for What is the distribution of handset prices for customers who own a computer and have a high income or incomegroup above 7?\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for What is the correlation between the number of referrals made by a subscriber and their revenue per minute?\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for Identify customers who have high SHAP values for credit rating and high monthly revenue but have low tenure. What are their details?\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for What is the average revenue per minute for customers who have made at least one call to the retention team and have accepted retention offers?\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for What are the key differences in usage patterns between customers who churned and those who did not\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for Idenitify customers with high monthly revenue (above 80th percentile is high)\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for Identify the top factors (SHAP values) contributing to churn for customers with high monthly revenue.\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for What is the relationship between tenure and churn rate for different income groups\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for Determine the impact of customer support interactions on churn by analyzing the number of customer care calls and churn rate.\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for Identify the most significant SHAP features for customers who have churned and have been with the company for more than 24 months.\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for Evaluate the churn rate and SHAP contributions for customers who own a motorcycle and have high monthly revenue.\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for Explain to me the main reasons for churn of customerID 3334558? Also explain what are the recommended actions to reduce churn for this customer\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for What would be impact of churn if the revenue per minute is decresed by 5c for customers with revenue per minute more than 1$\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for How would the Lifetime Value aka CLV change if the revenue per minute is decresed by 5c for customers with revenue per minute more than 1$ for customers in houston\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for What are the main reasons for churn for customers with Chidren and are home Owners\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for get the data of customers with children and are home owners\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for What would be impact of churn if  new equipment is provided for customers with currentequipmentdays more than 600 days\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for What would be change in CLV if monthlyrevenue is cut by 5 percent for customers with high churn probability\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for get the data of customers with overageminutes more than 50\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for what is churn pattern for customers with overage minutes more than 50\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for get the data of customers with occupation as homemaker\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for what are the different occupations customers have\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for what are the different credit ratings  customers have\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for what is distribution of creditrating among customers and their average churn rates\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Example SQL Embeddings for get me data of customers who have drastic change in minutes used\n",
      "Example SQL Embeddings for telecom_churn\n",
      "Done!!\n"
     ]
    }
   ],
   "source": [
    "# Load the file\n",
    "df_kgq = pd.read_csv(\"results\\\\tel_churn\\\\known_good_sql.csv\")\n",
    "df_kgq = df_kgq.loc[:, [\"prompt\", \"sql\", \"database_name\"]]\n",
    "df_kgq = df_kgq.dropna()\n",
    "\n",
    "\n",
    "# Add KGQ to the vector store\n",
    "await(store_kgq_embeddings(df_kgq,\n",
    "                            project_id=project_id,\n",
    "                            schema='telecom_churn'\n",
    "                            ))\n",
    "\n",
    "print('Done!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_chat_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
